{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "babylm_data_path =  \"/data/corpora/babylm/babylm_data/babylm_100M/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcorpora = {\n",
    "   \"open_subtitles\": \"Open Subtitles\",\n",
    "   \"qed\": \"QED\",\n",
    "   \"bnc_spoken\": \"BNC Spoken\",\n",
    "   \"wikipedia\": \"Wikipedia\",\n",
    "   \"gutenberg\": \"Gutenberg\",\n",
    "   \"aochildes\": \"CHILDES\",\n",
    "   \"simple_wikipedia\": \"Simple Wikipedia\",\n",
    "   \"children_stories\": \"Children's Stories\",\n",
    "   \"cbt\": \"Children's Book Test\",\n",
    "   \"switchboard\": \"Switchboard\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_subtitles, dropped 6062 lines, 0 tokens.\n",
      "qed, dropped 351 lines, 0 tokens.\n",
      "bnc_spoken, dropped 785 lines, 0 tokens.\n",
      "wikipedia, dropped 51989 lines, 0 tokens.\n",
      "gutenberg, dropped 231709 lines, 0 tokens.\n",
      "aochildes, dropped 1 lines, 0 tokens.\n",
      "simple_wikipedia, dropped 119549 lines, 0 tokens.\n",
      "children_stories, dropped 1381 lines, 0 tokens.\n",
      "cbt, dropped 1 lines, 0 tokens.\n",
      "switchboard, dropped 1 lines, 0 tokens.\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpora = {}\n",
    "\n",
    "total_tokens = 0\n",
    "total_tokens_dropped = 0\n",
    "\n",
    "for subcorpus in subcorpora:\n",
    "    subcorpus_train_file_path = os.path.join(babylm_data_path, f\"{subcorpus}.train\") \n",
    "    with open(subcorpus_train_file_path, \"r\") as f:\n",
    "        text = f.read().split(\"\\n\")\n",
    "        text = [line.split() for line in text]\n",
    "        num_tokens_before = sum([len(line) for line in text])\n",
    "        total_tokens += num_tokens_before\n",
    "        num_lines_before = len(text)\n",
    "        # we don't want sequences with less than 5 words\n",
    "        text = [line for line in text if len(line) >= 1]\n",
    "        num_tokens_after = sum([len(line) for line in text])\n",
    "        num_lines_after = len(text)\n",
    "        total_tokens_dropped += num_tokens_before-num_tokens_after\n",
    "        cleaned_corpora[subcorpus] = text\n",
    "        print(f\"{subcorpus}, dropped {num_lines_before-num_lines_after} lines, {num_tokens_before-num_tokens_after} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(cleaned_corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percantage of tokens dropped: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"Percantage of tokens dropped: {np.round(total_tokens_dropped/total_tokens, 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort sequences by length\n",
    "ordered_corpora = {subcorpus: sorted(text, key=lambda x: len(x)) for subcorpus, text in cleaned_corpora.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_subtitles 1 5.7567917122367405 436\n",
      "qed 1 10.672886990048456 2761\n",
      "bnc_spoken 1 9.617175093285232 1913\n",
      "wikipedia 1 49.62278091935452 926\n",
      "gutenberg 1 10.531864916975772 54\n",
      "aochildes 2 5.513832005434633 81\n",
      "simple_wikipedia 1 25.85618847066874 1674\n",
      "children_stories 1 42.117570044514274 1055\n",
      "cbt 1 21.05404543884881 251\n",
      "switchboard 1 7.297446519104736 107\n"
     ]
    }
   ],
   "source": [
    "for subcorpus, text in cleaned_corpora.items():\n",
    "    line_len = [len(line) for line in text]\n",
    "    print(subcorpus, np.min(line_len), np.mean(line_len), np.max(line_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcorpora_by_mean_seq_len = [\n",
    "    \"aochildes\",\n",
    "    \"open_subtitles\",\n",
    "    \"switchboard\",\n",
    "    \"gutenberg\",\n",
    "    \"bnc_spoken\",\n",
    "    \"qed\",\n",
    "    \"cbt\",\n",
    "    \"simple_wikipedia\",\n",
    "    \"wikipedia\",\n",
    "    \"children_stories\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = \"/data/corpora/babylm/babylm_data/babylm_100M\"\n",
    "\n",
    "# out_file_path = os.path.join(OUTPUT_PATH, \"babylm_min20_ordered_per_subcorpus.train.txt\")\n",
    "\n",
    "# with open(out_file_path, \"w\") as f:\n",
    "#     for subcorpus in subcorpora_by_mean_seq_len:\n",
    "#         for line in ordered_corpora[subcorpus]:\n",
    "#             if len(line) > BLOCK_SIZE:\n",
    "#                 num_splits = int(np.ceil(len(line)/BLOCK_SIZE))\n",
    "#                 for split_idx in range(BLOCK_SIZE,num_splits*BLOCK_SIZE+1,BLOCK_SIZE):\n",
    "#                     split_line = line[split_idx-BLOCK_SIZE:split_idx]\n",
    "#                     out_str = \" \".join(split_line) + \"\\n\"\n",
    "#                     f.write(out_str)\n",
    "#             else:\n",
    "#                 out_str = \" \".join(line) + \"\\n\"\n",
    "#                 f.write(out_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = \"/data/corpora/babylm/babylm_data/babylm_100M\"\n",
    "\n",
    "# out_file_path = os.path.join(OUTPUT_PATH, \"babylm_min20.train.txt\")\n",
    "\n",
    "# with open(out_file_path, \"w\") as f:\n",
    "#     for subcorpus in subcorpora_by_mean_seq_len:\n",
    "#         for line in cleaned_corpora[subcorpus]:\n",
    "#             if len(line) > BLOCK_SIZE:\n",
    "#                 num_splits = int(np.ceil(len(line)/BLOCK_SIZE))\n",
    "#                 for split_idx in range(BLOCK_SIZE,num_splits*BLOCK_SIZE+1,BLOCK_SIZE):\n",
    "#                     split_line = line[split_idx-BLOCK_SIZE:split_idx]\n",
    "#                     out_str = \" \".join(split_line) + \"\\n\"\n",
    "#                     f.write(out_str)\n",
    "#             else:\n",
    "#                 out_str = \" \".join(line) + \"\\n\"\n",
    "#                 f.write(out_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# text_all_corpora = list(chain.from_iterable([text for text in ordered_corpora.values()]))\n",
    "# text_all_corpora = sorted(text_all_corpora, key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_PATH = \"/data/corpora/babylm/babylm_data/babylm_100M\"\n",
    "\n",
    "# out_file_path = os.path.join(OUTPUT_PATH, \"babylm_ordered.train.txt\")\n",
    "\n",
    "# with open(out_file_path, \"w\") as f:\n",
    "#     for line in text_all_corpora:\n",
    "#         if len(line) > BLOCK_SIZE:\n",
    "#             num_splits = int(np.ceil(len(line)/BLOCK_SIZE))\n",
    "#             for split_idx in range(BLOCK_SIZE,num_splits*BLOCK_SIZE+1,BLOCK_SIZE):\n",
    "#                 split_line = line[split_idx-BLOCK_SIZE:split_idx]\n",
    "#                 out_str = \" \".join(split_line) + \"\\n\"\n",
    "#                 f.write(out_str)\n",
    "#         else:\n",
    "#             out_str = \" \".join(line) + \"\\n\"\n",
    "#             f.write(out_str)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aochildes\n",
      "open_subtitles\n",
      "switchboard\n",
      "gutenberg\n",
      "bnc_spoken\n",
      "qed\n",
      "cbt\n",
      "simple_wikipedia\n",
      "wikipedia\n",
      "children_stories\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = \"/data/corpora/babylm/babylm_data/babylm_100M\"\n",
    "\n",
    "out_file_path = os.path.join(OUTPUT_PATH, \"babylm_by_subcorpus.train.txt\")\n",
    "\n",
    "with open(out_file_path, \"w\") as f:\n",
    "    for subcorpus in subcorpora_by_mean_seq_len:\n",
    "        print(subcorpus)\n",
    "        text = cleaned_corpora[subcorpus]\n",
    "        for line in text:\n",
    "            if len(line) > BLOCK_SIZE:\n",
    "                num_splits = int(np.ceil(len(line)/BLOCK_SIZE))\n",
    "                for split_idx in range(BLOCK_SIZE,num_splits*BLOCK_SIZE+1,BLOCK_SIZE):\n",
    "                    split_line = line[split_idx-BLOCK_SIZE:split_idx]\n",
    "                    out_str = \" \".join(split_line) + \"\\n\"\n",
    "                    f.write(out_str)\n",
    "            else:\n",
    "                out_str = \" \".join(line) + \"\\n\"\n",
    "                f.write(out_str)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "babylm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
